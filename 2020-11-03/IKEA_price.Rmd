---
title: "Predicting the Price of IKEA Furniture"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, dpi = 180)
library(tidyverse)
```

For this [TidyTuesday](https://github.com/rfordatascience/tidytuesday) dataset, we will be looking at IKEA furniture prices. We thus aim to predic the price of IKEA furniture from other furniture features such as category and size. This analysis is motivated by [Julia Silge's screencast](https://juliasilge.com/blog/ikea-prices/).

```{r, echo = FALSE}
ikea <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-11-03/ikea.csv")
```

## Exploratory Data Analysis
It is common to look at log values of the a parameter whose magnitude is in thousands such as price
```{r, echo = FALSE}
ikea %>% 
  select(X1, price, depth:width) %>% 
  pivot_longer(depth:width, names_to = "dim") %>% 
  ggplot(aes(value, price, color = dim)) + 
  geom_point(alpha = 0.4, show.legend = FALSE) +
  scale_y_log10() +
  facet_wrap(~ dim, scales = "free_x") +
  labs(x = NULL)
```
- it seems width has the strongest relationship with price

- reasonable observation includes bigger objects have higher price

```{r}
ikea_df <- ikea %>% 
  select(price, name, category, depth, height, width) %>% 
  mutate(price = log10(price)) %>% 
  mutate_if(is.character, factor)
```

## Build a predictive model

```{r}
library(tidymodels)

set.seed(123)
ikea_split <- initial_split(ikea_df, strata = price)
ikea_train <- training(ikea_split)
ikea_test <- testing(ikea_split)

# set resamples for tuning of our model
set.seed(234)
ikea_folds <- bootstraps(ikea_train, strata = price)
ikea_folds
```

`usemodels` package can be very helpful here because it provides scaffolding for getting started with tidymodels tuning. It will yield a good standard default workflow. It requires two inputs:
- a formula to describe our model, just like `lm`, `price ~.`

- our training data

```{r}
library(usemodels)
library(textrecipes)
use_ranger(price ~ ., data = ikea_train)

ranger_recipe <- 
  recipe(formula = price ~ ., data = ikea_train) %>% 
  # name's levels are too many
  step_other(name, category, threshold = 0.01) %>% 
  # some names have punctuations, different cases
  step_clean_levels(name, category) %>% 
  # there are too many NA's --> instead of removing these data, we impute
  step_knnimpute(depth, height, width)

ranger_spec <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_mode("regression") %>% 
  set_engine("ranger") 

ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec) 

set.seed(91010)
doParallel::registerDoParallel()
ranger_tune <-
  tune_grid(ranger_workflow, 
            resamples = ikea_folds,
            grid = 11)
```
The `usemodels` required us to input manually the `resamples` and `grid` parameters.

## Explore Results
Let's check the best-performing models in the tuning results

```{r}
show_best(ranger_tune, metric = 'rmse')

show_best(ranger_tune, metric = 'rsq')
```

Let's now see on the performance of all the possible paramater combinations
```{r}
autoplot(ranger_tune)
```

* regular autoplot will usually show curvey trends, instead of sparse grid like what we have here.

With the best-performing parameters, let's update our random forest workflow
```{r}
final_rf <- ranger_workflow %>% 
  finalize_workflow(select_best(ranger_tune))

final_rf
```

```{r}
ikea_fit <- last_fit(final_rf, ikea_split)
ikea_fit
```

```{r}
collect_metrics(ikea_fit)
```

```{r}
collect_predictions(ikea_fit) %>% 
  ggplot(aes(price, .pred)) +
  geom_abline(lty = 2, color = "gray50") + 
  geom_point(alpha = 0.5, color = "midnightblue")
```

- we do better on cheaper things

We can use this model to predict other data e.g.
```{r}
predict(ikea_fit$.workflow[[1]], ikea_test[15,])
```

```{r}
library(vip)

imp_spec <- ranger_spec %>% 
  finalize_model(select_best(ranger_tune)) %>% 
  set_engine("ranger", importance = "permutation")

workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(imp_spec) %>% 
  fit(ikea_train) %>% 
  pull_workflow_fit() %>% 
  vip(aesthetics = list(alpha = 0.8, fill = 'midnightblue'))
```

- `width` is the most important feature which is what we observed earlier during exploratory data analysis

- `name` is the least important which is probably because we don't have enough data.

- `category` surprisingly has low importance. It is usually easy to roughly guess the price of an item based on its category.

Let's try running different algorithm such as xgboost

```{r}
use_xgboost(price ~., data = ikea_train)
xgboost_recipe <- 
  recipe(formula = price ~ ., data = ikea_train) %>% 
  # name's levels are too many
  step_other(name, category, threshold = 0.01) %>% 
  # some names have punctuations, different cases
  step_clean_levels(name, category) %>% 
  # there are too many NA's --> instead of removing these data, we impute
  step_knnimpute(depth, height, width) %>%
  step_novel(all_nominal(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
  step_zv(all_predictors())

xgboost_spec <- 
  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), 
    loss_reduction = tune(), sample_size = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("xgboost") 

xgboost_workflow <- 
  workflow() %>% 
  add_recipe(xgboost_recipe) %>% 
  add_model(xgboost_spec) 

doParallel::registerDoParallel()
set.seed(41860)
xgboost_tune <-
  tune_grid(xgboost_workflow, 
            resamples = ikea_folds, 
            grid = 4)
```

```{r}
autoplot(xgboost_tune)
```
Since the autoplot suggest that there's no best-performing model from xgboost, we may want to fine tune the parameters to ensure our model runs properly.
